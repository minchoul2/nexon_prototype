{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_1 = pd.read_csv('../input/payco_23.csv')\n",
    "data_2 = pd.read_csv('../input/payco_2304.csv')\n",
    "df = pd.concat([data_1,data_2])\n",
    "df = df[['사원번호','사용처']].rename({'사원번호':'userid', '사용처':'itemid'}, axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create user-item matrix\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Create a new DataFrame with frequency count for each user-item pair\n",
    "df_grouped = df.groupby(['userid', 'itemid']).size().reset_index(name='frequency')\n",
    "\n",
    "user_u = list(sorted(df_grouped.userid.unique()))\n",
    "item_u = list(sorted(df_grouped.itemid.unique()))\n",
    "\n",
    "user_c = CategoricalDtype(sorted(df_grouped['userid'].unique()), ordered=True)\n",
    "item_c = CategoricalDtype(sorted(df_grouped['itemid'].unique()), ordered=True)\n",
    "\n",
    "row = df_grouped['userid'].astype(user_c).cat.codes\n",
    "col = df_grouped['itemid'].astype(item_c).cat.codes\n",
    "data = df_grouped['frequency'].tolist()\n",
    "\n",
    "sparse_matrix = csr_matrix((data, (row, col)), shape=(len(user_u), len(item_u)))\n",
    "\n",
    "df_user_item = pd.DataFrame.sparse.from_spmatrix(sparse_matrix, index=user_u, columns=item_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define AutoRec model\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class AutoRec(nn.Module):\n",
    "    def __init__(self, num_inputs, hidden_units):\n",
    "        super(AutoRec, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Linear(num_inputs, hidden_units)\n",
    "        self.decoder = nn.Linear(hidden_units, num_inputs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.encoder(x))\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Train and Test AutoRec model\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "hidden_units = 500\n",
    "num_epochs = 100\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "\n",
    "model = AutoRec(df_user_item.shape[1], hidden_units).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "data = torch.FloatTensor(df_user_item.values).to(device)\n",
    "dataset = TensorDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.1675979197025299\n",
      "Epoch 2/100, Loss: 0.1058153510093689\n",
      "Epoch 3/100, Loss: 0.06619088351726532\n",
      "Epoch 4/100, Loss: 0.05609368532896042\n",
      "Epoch 5/100, Loss: 0.042968522757291794\n",
      "Epoch 6/100, Loss: 0.03719198331236839\n",
      "Epoch 7/100, Loss: 0.017582252621650696\n",
      "Epoch 8/100, Loss: 0.011145561933517456\n",
      "Epoch 9/100, Loss: 0.013153713196516037\n",
      "Epoch 10/100, Loss: 0.010720187798142433\n",
      "Epoch 11/100, Loss: 0.0063661918975412846\n",
      "Epoch 12/100, Loss: 0.007771084550768137\n",
      "Epoch 13/100, Loss: 0.006385880522429943\n",
      "Epoch 14/100, Loss: 0.00468917703256011\n",
      "Epoch 15/100, Loss: 0.005302347242832184\n",
      "Epoch 16/100, Loss: 0.004053828772157431\n",
      "Epoch 17/100, Loss: 0.007856742478907108\n",
      "Epoch 18/100, Loss: 0.005359386559575796\n",
      "Epoch 19/100, Loss: 0.01242919359356165\n",
      "Epoch 20/100, Loss: 0.0037751856725662947\n",
      "Epoch 21/100, Loss: 0.008395659737288952\n",
      "Epoch 22/100, Loss: 0.0026499277446419\n",
      "Epoch 23/100, Loss: 0.002478748792782426\n",
      "Epoch 24/100, Loss: 0.002113355090841651\n",
      "Epoch 25/100, Loss: 0.004415900446474552\n",
      "Epoch 26/100, Loss: 0.0015859486302360892\n",
      "Epoch 27/100, Loss: 0.006161426194012165\n",
      "Epoch 28/100, Loss: 0.002738297451287508\n",
      "Epoch 29/100, Loss: 0.0029803025536239147\n",
      "Epoch 30/100, Loss: 0.002310012001544237\n",
      "Epoch 31/100, Loss: 0.0016108492854982615\n",
      "Epoch 32/100, Loss: 0.0015145387733355165\n",
      "Epoch 33/100, Loss: 0.0013888541143387556\n",
      "Epoch 34/100, Loss: 0.004038970917463303\n",
      "Epoch 35/100, Loss: 0.0012864999007433653\n",
      "Epoch 36/100, Loss: 0.0012575008440762758\n",
      "Epoch 37/100, Loss: 0.0015204186784103513\n",
      "Epoch 38/100, Loss: 0.0023474381305277348\n",
      "Epoch 39/100, Loss: 0.0008903547422960401\n",
      "Epoch 40/100, Loss: 0.0009318063384853303\n",
      "Epoch 41/100, Loss: 0.0017175357788801193\n",
      "Epoch 42/100, Loss: 0.0016254906076937914\n",
      "Epoch 43/100, Loss: 0.0012971031246706843\n",
      "Epoch 44/100, Loss: 0.0014691543765366077\n",
      "Epoch 45/100, Loss: 0.0009775995276868343\n",
      "Epoch 46/100, Loss: 0.001390470890328288\n",
      "Epoch 47/100, Loss: 0.0007900860509835184\n",
      "Epoch 48/100, Loss: 0.0015375757357105613\n",
      "Epoch 49/100, Loss: 0.0007400893955491483\n",
      "Epoch 50/100, Loss: 0.0010713115334510803\n",
      "Epoch 51/100, Loss: 0.0030020063277333975\n",
      "Epoch 52/100, Loss: 0.0010934557067230344\n",
      "Epoch 53/100, Loss: 0.0009487426141276956\n",
      "Epoch 54/100, Loss: 0.0008578106644563377\n",
      "Epoch 55/100, Loss: 0.0014104116708040237\n",
      "Epoch 56/100, Loss: 0.000807245378382504\n",
      "Epoch 57/100, Loss: 0.0007140121888369322\n",
      "Epoch 58/100, Loss: 0.0009439849527552724\n",
      "Epoch 59/100, Loss: 0.0006985071231611073\n",
      "Epoch 60/100, Loss: 0.0006312921177595854\n",
      "Epoch 61/100, Loss: 0.0006839408888481557\n",
      "Epoch 62/100, Loss: 0.0006107600638642907\n",
      "Epoch 63/100, Loss: 0.0011766268871724606\n",
      "Epoch 64/100, Loss: 0.00196798425167799\n",
      "Epoch 65/100, Loss: 0.0021443618461489677\n",
      "Epoch 66/100, Loss: 0.0006462252931669354\n",
      "Epoch 67/100, Loss: 0.0005856052739545703\n",
      "Epoch 68/100, Loss: 0.0006153591675683856\n",
      "Epoch 69/100, Loss: 0.0008366063120774925\n",
      "Epoch 70/100, Loss: 0.0005944661679677665\n",
      "Epoch 71/100, Loss: 0.0007337515125982463\n",
      "Epoch 72/100, Loss: 0.0009342667181044817\n",
      "Epoch 73/100, Loss: 0.001049605431035161\n",
      "Epoch 74/100, Loss: 0.0006047669448889792\n",
      "Epoch 75/100, Loss: 0.0007742525776848197\n",
      "Epoch 76/100, Loss: 0.00047082046512514353\n",
      "Epoch 77/100, Loss: 0.0011177259730175138\n",
      "Epoch 78/100, Loss: 0.0005196182173676789\n",
      "Epoch 79/100, Loss: 0.0016719123814255\n",
      "Epoch 80/100, Loss: 0.0007961343508213758\n",
      "Epoch 81/100, Loss: 0.000495242711622268\n",
      "Epoch 82/100, Loss: 0.00046848063357174397\n",
      "Epoch 83/100, Loss: 0.0007219602121040225\n",
      "Epoch 84/100, Loss: 0.0005393748870119452\n",
      "Epoch 85/100, Loss: 0.0005105237942188978\n",
      "Epoch 86/100, Loss: 0.0006097297300584614\n",
      "Epoch 87/100, Loss: 0.0013172118924558163\n",
      "Epoch 88/100, Loss: 0.0006948037189431489\n",
      "Epoch 89/100, Loss: 0.0004443144134711474\n",
      "Epoch 90/100, Loss: 0.0003864031459670514\n",
      "Epoch 91/100, Loss: 0.00047597417142242193\n",
      "Epoch 92/100, Loss: 0.0006928920629434288\n",
      "Epoch 93/100, Loss: 0.000442957243649289\n",
      "Epoch 94/100, Loss: 0.0004890899290330708\n",
      "Epoch 95/100, Loss: 0.0005672024562954903\n",
      "Epoch 96/100, Loss: 0.0005129885394126177\n",
      "Epoch 97/100, Loss: 0.000695491733495146\n",
      "Epoch 98/100, Loss: 0.0005364486132748425\n",
      "Epoch 99/100, Loss: 0.0010092278243973851\n",
      "Epoch 100/100, Loss: 0.0005239385645836592\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs,) in enumerate(dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.8427e-03, -4.7674e-03, -1.2007e-03,  ..., -1.2878e-02,\n",
      "         -1.5792e-03,  1.9722e-03],\n",
      "        [ 1.3305e-03, -5.4564e-03, -6.8760e-03,  ..., -1.5616e-02,\n",
      "         -3.1894e-03,  4.6495e-05],\n",
      "        [ 2.8625e-03, -9.9125e-03, -1.0187e-03,  ..., -1.2099e-02,\n",
      "         -3.4488e-03,  3.7934e-03],\n",
      "        ...,\n",
      "        [-6.1278e-04,  3.1359e-02,  1.1385e-02,  ..., -1.1529e-02,\n",
      "         -4.4342e-04,  1.3491e-02],\n",
      "        [ 4.0145e-03, -1.8813e-02, -6.6315e-03,  ..., -1.2514e-02,\n",
      "         -1.7078e-03,  3.6300e-03],\n",
      "        [ 3.6387e-03,  3.4081e-02, -7.7633e-03,  ..., -1.2935e-02,\n",
      "         -9.5587e-03,  8.4596e-03]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    inputs = data\n",
    "    outputs = model(inputs)\n",
    "    print(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Generate recommendations\n",
    "import numpy as np\n",
    "\n",
    "def user_free_inference(items, df_user_item, model, top_k=10):\n",
    "    # Create a new user vector\n",
    "    user_vector = np.zeros(df_user_item.shape[1])\n",
    "    item_indices = []\n",
    "\n",
    "    # Set the chosen items to the maximum value\n",
    "    for item in items:\n",
    "        if item in df_user_item.columns:\n",
    "            item_index = df_user_item.columns.get_loc(item)\n",
    "            user_vector[item_index] = df_user_item.values.max()\n",
    "            item_indices.append(item_index)\n",
    "        else:\n",
    "            raise ValueError(f\"Item {item} not found in the data\")\n",
    "\n",
    "    # Convert to tensor and move to the correct device\n",
    "    user_vector = torch.FloatTensor([user_vector]).to(device)\n",
    "\n",
    "    # Generate recommendations\n",
    "    with torch.no_grad():\n",
    "        outputs = model(user_vector)\n",
    "        predicted_ratings = outputs.cpu().numpy()[0]\n",
    "\n",
    "    # Remove the chosen items from the predictions\n",
    "    predicted_ratings[item_indices] = -np.inf\n",
    "\n",
    "    top_k_item_indices = np.argsort(-predicted_ratings)[:top_k]\n",
    "    recommended_items = df_user_item.columns[top_k_item_indices]\n",
    "    recommended_scores = predicted_ratings[top_k_item_indices]\n",
    "\n",
    "    # Convert item and score to dictionary\n",
    "    item_score_dict = dict(zip(recommended_items.tolist(), recommended_scores.tolist()))\n",
    "\n",
    "    # Print each item and it score\n",
    "    for item, score in item_score_dict.items():\n",
    "        print(f\"{item}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(주)엔바이콘 판교순대: 2.1938366889953613\n",
      "써브웨이(판교브릿지타워점): 1.1317442655563354\n",
      "봉추찜닭(판교테크노밸리점): 0.9974612593650818\n",
      "제주은희네해장국(판교점): 0.9058394432067871\n",
      "(주)엔바이콘 하림닭요리: 0.8551080822944641\n",
      "(주)엔바이콘 왕스덕: 0.758247435092926\n",
      "듬박이찌개(판교점): 0.7211755514144897\n",
      "(주)오전오후: 0.48437389731407166\n",
      "일류(판교점): 0.45968949794769287\n",
      "쿠차라(판교카카오점): 0.409889280796051\n"
     ]
    }
   ],
   "source": [
    "# Get recommendations\n",
    "item_list = [\n",
    "    '킨파',\n",
    "    '서호돈가스',\n",
    "    '버거킹(판교유스페이스)',\n",
    "    '일상화식'\n",
    "]\n",
    "\n",
    "user_free_inference(item_list, df_user_item, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
