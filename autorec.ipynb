{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_1 = pd.read_csv('./input/payco_23.csv')\n",
    "data_2 = pd.read_csv('./input/payco_2304.csv')\n",
    "df = pd.concat([data_1,data_2])\n",
    "df = df[['사원번호','사용처']].rename({'사원번호':'userid', '사용처':'itemid'}, axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create user-item matrix\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Create a new DataFrame with frequency count for each user-item pair\n",
    "df_grouped = df.groupby(['userid', 'itemid']).size().reset_index(name='frequency')\n",
    "\n",
    "user_u = list(sorted(df_grouped.userid.unique()))\n",
    "item_u = list(sorted(df_grouped.itemid.unique()))\n",
    "\n",
    "user_c = CategoricalDtype(sorted(df_grouped['userid'].unique()), ordered=True)\n",
    "item_c = CategoricalDtype(sorted(df_grouped['itemid'].unique()), ordered=True)\n",
    "\n",
    "row = df_grouped['userid'].astype(user_c).cat.codes\n",
    "col = df_grouped['itemid'].astype(item_c).cat.codes\n",
    "data = df_grouped['frequency'].tolist()\n",
    "\n",
    "sparse_matrix = csr_matrix((data, (row, col)), shape=(len(user_u), len(item_u)))\n",
    "\n",
    "df_user_item = pd.DataFrame.sparse.from_spmatrix(sparse_matrix, index=user_u, columns=item_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define AutoRec model\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class AutoRec(nn.Module):\n",
    "    def __init__(self, num_inputs, hidden_units):\n",
    "        super(AutoRec, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Linear(num_inputs, hidden_units)\n",
    "        self.decoder = nn.Linear(hidden_units, num_inputs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.encoder(x))\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Train and Test AutoRec model\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "hidden_units = 500\n",
    "num_epochs = 100\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "\n",
    "model = AutoRec(df_user_item.shape[1], hidden_units).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "data = torch.FloatTensor(df_user_item.values).to(device)\n",
    "dataset = TensorDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.17751044034957886\n",
      "Epoch 2/100, Loss: 0.10845330357551575\n",
      "Epoch 3/100, Loss: 0.0975332260131836\n",
      "Epoch 4/100, Loss: 0.05658097192645073\n",
      "Epoch 5/100, Loss: 0.031433962285518646\n",
      "Epoch 6/100, Loss: 0.09726953506469727\n",
      "Epoch 7/100, Loss: 0.03251245990395546\n",
      "Epoch 8/100, Loss: 0.018965357914566994\n",
      "Epoch 9/100, Loss: 0.015315351076424122\n",
      "Epoch 10/100, Loss: 0.009196609258651733\n",
      "Epoch 11/100, Loss: 0.010769040323793888\n",
      "Epoch 12/100, Loss: 0.009781679138541222\n",
      "Epoch 13/100, Loss: 0.005064076744019985\n",
      "Epoch 14/100, Loss: 0.007658516056835651\n",
      "Epoch 15/100, Loss: 0.005749749951064587\n",
      "Epoch 16/100, Loss: 0.00856492854654789\n",
      "Epoch 17/100, Loss: 0.0035773131530731916\n",
      "Epoch 18/100, Loss: 0.0034561119973659515\n",
      "Epoch 19/100, Loss: 0.0032567933667451143\n",
      "Epoch 20/100, Loss: 0.007260635029524565\n",
      "Epoch 21/100, Loss: 0.0023437871132045984\n",
      "Epoch 22/100, Loss: 0.0031590082217007875\n",
      "Epoch 23/100, Loss: 0.008594038896262646\n",
      "Epoch 24/100, Loss: 0.0021084053441882133\n",
      "Epoch 25/100, Loss: 0.005517830140888691\n",
      "Epoch 26/100, Loss: 0.002324603032320738\n",
      "Epoch 27/100, Loss: 0.002410848392173648\n",
      "Epoch 28/100, Loss: 0.001841756864450872\n",
      "Epoch 29/100, Loss: 0.0021641829516738653\n",
      "Epoch 30/100, Loss: 0.002127349143847823\n",
      "Epoch 31/100, Loss: 0.0014029074227437377\n",
      "Epoch 32/100, Loss: 0.005159385967999697\n",
      "Epoch 33/100, Loss: 0.0017492850311100483\n",
      "Epoch 34/100, Loss: 0.0016999254003167152\n",
      "Epoch 35/100, Loss: 0.002406158484518528\n",
      "Epoch 36/100, Loss: 0.0016368887154385448\n",
      "Epoch 37/100, Loss: 0.0030264905653893948\n",
      "Epoch 38/100, Loss: 0.0016817997675389051\n",
      "Epoch 39/100, Loss: 0.0016157699283212423\n",
      "Epoch 40/100, Loss: 0.0011753668077290058\n",
      "Epoch 41/100, Loss: 0.0019249997567385435\n",
      "Epoch 42/100, Loss: 0.0010847236262634397\n",
      "Epoch 43/100, Loss: 0.0010465001687407494\n",
      "Epoch 44/100, Loss: 0.0013088606065139174\n",
      "Epoch 45/100, Loss: 0.0009532002732157707\n",
      "Epoch 46/100, Loss: 0.0009409759659320116\n",
      "Epoch 47/100, Loss: 0.0011491633486002684\n",
      "Epoch 48/100, Loss: 0.0008834745385684073\n",
      "Epoch 49/100, Loss: 0.0013385217171162367\n",
      "Epoch 50/100, Loss: 0.0010125734843313694\n",
      "Epoch 51/100, Loss: 0.0008220489253289998\n",
      "Epoch 52/100, Loss: 0.0007669749320484698\n",
      "Epoch 53/100, Loss: 0.000845396367367357\n",
      "Epoch 54/100, Loss: 0.0011637303978204727\n",
      "Epoch 55/100, Loss: 0.0008104072767309844\n",
      "Epoch 56/100, Loss: 0.0018428696785122156\n",
      "Epoch 57/100, Loss: 0.0020007893908768892\n",
      "Epoch 58/100, Loss: 0.0011025264393538237\n",
      "Epoch 59/100, Loss: 0.000844141177367419\n",
      "Epoch 60/100, Loss: 0.0008080620900727808\n",
      "Epoch 61/100, Loss: 0.0008271721308119595\n",
      "Epoch 62/100, Loss: 0.001375923864543438\n",
      "Epoch 63/100, Loss: 0.000911158393137157\n",
      "Epoch 64/100, Loss: 0.0007859902689233422\n",
      "Epoch 65/100, Loss: 0.0011860820231959224\n",
      "Epoch 66/100, Loss: 0.00062251539202407\n",
      "Epoch 67/100, Loss: 0.0006153805879876018\n",
      "Epoch 68/100, Loss: 0.0005711486446671188\n",
      "Epoch 69/100, Loss: 0.0006876337574794888\n",
      "Epoch 70/100, Loss: 0.0005642674514092505\n",
      "Epoch 71/100, Loss: 0.0005461358814500272\n",
      "Epoch 72/100, Loss: 0.0005482816486619413\n",
      "Epoch 73/100, Loss: 0.0005943727446720004\n",
      "Epoch 74/100, Loss: 0.00131103559397161\n",
      "Epoch 75/100, Loss: 0.0006147997337393463\n",
      "Epoch 76/100, Loss: 0.0006373272626660764\n",
      "Epoch 77/100, Loss: 0.0005189716466702521\n",
      "Epoch 78/100, Loss: 0.0007421494810841978\n",
      "Epoch 79/100, Loss: 0.0005295034497976303\n",
      "Epoch 80/100, Loss: 0.0007793732220306993\n",
      "Epoch 81/100, Loss: 0.0006635679164901376\n",
      "Epoch 82/100, Loss: 0.0006171171553432941\n",
      "Epoch 83/100, Loss: 0.0006172172143124044\n",
      "Epoch 84/100, Loss: 0.0005851710448041558\n",
      "Epoch 85/100, Loss: 0.0005097773391753435\n",
      "Epoch 86/100, Loss: 0.0005488941096700728\n",
      "Epoch 87/100, Loss: 0.0009944725316017866\n",
      "Epoch 88/100, Loss: 0.0014079263200983405\n",
      "Epoch 89/100, Loss: 0.0005744508234784007\n",
      "Epoch 90/100, Loss: 0.0004868122923653573\n",
      "Epoch 91/100, Loss: 0.001757423859089613\n",
      "Epoch 92/100, Loss: 0.00044375783181749284\n",
      "Epoch 93/100, Loss: 0.0006493896944448352\n",
      "Epoch 94/100, Loss: 0.0012157309101894498\n",
      "Epoch 95/100, Loss: 0.0005803030217066407\n",
      "Epoch 96/100, Loss: 0.0006373757496476173\n",
      "Epoch 97/100, Loss: 0.0011353547452017665\n",
      "Epoch 98/100, Loss: 0.0004675116506405175\n",
      "Epoch 99/100, Loss: 0.000471843290142715\n",
      "Epoch 100/100, Loss: 0.0004743212484754622\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs,) in enumerate(dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0107,  0.0004, -0.0245,  ..., -0.0036, -0.0033,  0.0127],\n",
      "        [ 0.0117,  0.0085, -0.0224,  ..., -0.0070, -0.0041,  0.0104],\n",
      "        [ 0.0124, -0.0017, -0.0269,  ..., -0.0041, -0.0018,  0.0110],\n",
      "        ...,\n",
      "        [ 0.0124,  0.0393, -0.0340,  ...,  0.0030,  0.0040,  0.0100],\n",
      "        [ 0.0132, -0.0047, -0.0287,  ..., -0.0029, -0.0035,  0.0104],\n",
      "        [ 0.0136,  0.0089,  0.0094,  ..., -0.0087, -0.0093,  0.0099]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    inputs = data\n",
    "    outputs = model(inputs)\n",
    "    print(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Generate recommendations\n",
    "import numpy as np\n",
    "\n",
    "def user_free_inference(items, df_user_item, model, top_k=10):\n",
    "    # Create a new user vector\n",
    "    user_vector = np.zeros(df_user_item.shape[1])\n",
    "    item_indices = []\n",
    "    \n",
    "    # Set the chosen items to the maximum value\n",
    "    for item in items:\n",
    "        if item in df_user_item.columns:\n",
    "            item_index = df_user_item.columns.get_loc(item)\n",
    "            user_vector[item_index] = df_user_item.values.max()\n",
    "            item_indices.append(item_index)\n",
    "\n",
    "    # Convert to tensor and move to the correct device\n",
    "    user_vector = torch.FloatTensor([user_vector]).to(device)\n",
    "    \n",
    "    # Generate recommendations\n",
    "    with torch.no_grad():\n",
    "        outputs = model(user_vector)\n",
    "        predicted_ratings = outputs.cpu().numpy()[0]\n",
    "\n",
    "    # Remove the chosen items from the predictions\n",
    "    predicted_ratings[item_indices] = -np.inf\n",
    "\n",
    "    top_k_item_indices = np.argsort(-predicted_ratings)[:top_k]\n",
    "    recommended_items = df_user_item.columns[top_k_item_indices]\n",
    "    recommended_scores = predicted_ratings[top_k_item_indices]\n",
    "\n",
    "    # Convert item and score to dictionary\n",
    "    item_score_dict = dict(zip(recommended_items.tolist(), recommended_scores.tolist()))\n",
    "\n",
    "    return item_score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'킨파': 1.2579219341278076, '제주은희네해장국(판교점)': 1.2477407455444336, '(주)엔바이콘 판교순대': 1.152186393737793, '(주)엔바이콘 하이포크': 0.8855391144752502, '(주)엔바이콘 혼키라멘': 0.8552912473678589, '맥도날드(판교테크노밸리점)': 0.7202772498130798, '차이나오라': 0.6833038926124573, '하코야(판교우림점)': 0.6160076856613159, '춘업순대국과 0627 부대찌개': 0.5844380855560303, '조마루감자탕(판교점)': 0.5349248647689819}\n"
     ]
    }
   ],
   "source": [
    "# Get recommendations for user with ID 1\n",
    "print(user_free_inference(['최고야 전국5대짬뽕','버거킹(판교유스페이스)','서호돈가스'\n",
    "                           ], \n",
    "                           df_user_item,\n",
    "                            model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
